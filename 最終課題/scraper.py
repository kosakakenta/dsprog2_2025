"""
scraper.py
SUUMOã‹ã‚‰è³ƒè²¸ç‰©ä»¶ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆæ–°å®¿åŒºãƒ»ä¸–ç”°è°·åŒºå°‚ç”¨ï¼‰
"""

import requests
from bs4 import BeautifulSoup
import time
from typing import List, Dict
import re


class SuumoScraper:
    """SUUMOã‹ã‚‰ä¸å‹•ç”£ãƒ‡ãƒ¼ã‚¿å–å¾—"""
    
    # â˜…ä¿®æ­£: æ¸‹è°·åŒºã‚’å‰Šé™¤
    AREA_CODES = {
        'æ–°å®¿åŒº': '13104',
        'ä¸–ç”°è°·åŒº': '13112'
    }
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def scrape_area(self, area_code: str, area_name: str, pages: int = 3) -> List[Dict]:
        """
        æŒ‡å®šã‚¨ãƒªã‚¢ã®è³ƒè²¸ç‰©ä»¶å–å¾—
        
        Args:
            area_code: ã‚¨ãƒªã‚¢ã‚³ãƒ¼ãƒ‰ï¼ˆä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ãƒ»äº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰
            area_name: ã‚¨ãƒªã‚¢åï¼ˆæ–°å®¿åŒºã€ä¸–ç”°è°·åŒºã®ã¿å¯¾å¿œï¼‰
            pages: å–å¾—ãƒšãƒ¼ã‚¸æ•°
            
        Returns:
            ç‰©ä»¶ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        
        print(f"\nğŸ  SUUMO: {area_name}ã®ç‰©ä»¶ã‚’å–å¾—ä¸­...")
        
        # ã‚¨ãƒªã‚¢ã‚³ãƒ¼ãƒ‰å–å¾—
        suumo_area_code = self.AREA_CODES.get(area_name)
        
        if not suumo_area_code:
            print(f"   âŒ ã‚¨ãƒªã‚¢ '{area_name}' ã¯æœªå¯¾å¿œã§ã™")
            print(f"   â„¹ï¸  å¯¾å¿œã‚¨ãƒªã‚¢: {list(self.AREA_CODES.keys())}")
            return []
        
        properties = []
        
        for page in range(1, pages + 1):
            # ã‚¨ãƒªã‚¢åˆ¥URLæ§‹ç¯‰
            url = (
                f"https://suumo.jp/jj/chintai/ichiran/FR301FC001/"
                f"?ar=030&bs=040&ta=13&sc={suumo_area_code}&page={page}"
            )
            
            print(f"   ãƒšãƒ¼ã‚¸ {page}/{pages}...")
            print(f"   â³ 3ç§’å¾…æ©Ÿä¸­ï¼ˆã‚µãƒ¼ãƒè² è·è»½æ¸›ï¼‰...")
            time.sleep(3)  # â˜…å¿…é ˆï¼šåˆ©ç”¨è¦ç´„éµå®ˆ
            
            try:
                response = requests.get(url, headers=self.headers, timeout=10)
                
                if response.status_code != 200:
                    print(f"   âŒ HTTP {response.status_code}")
                    continue
                
                # HTMLè§£æ
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ç‰©ä»¶ã‚«ã‚»ãƒƒãƒˆå–å¾—
                cassettos = soup.find_all('div', class_='cassetteitem')
                
                if not cassettos:
                    print(f"   âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    continue
                
                print(f"   ğŸ“ {len(cassettos)}ä»¶æ¤œå‡º")
                
                for cassetto in cassettos:
                    try:
                        # ç‰©ä»¶å
                        title = cassetto.find('div', class_='cassetteitem_content-title')
                        if not title:
                            continue
                        name = title.get_text(strip=True)
                        
                        # ä½æ‰€
                        address_tag = cassetto.find('li', class_='cassetteitem_detail-col1')
                        address = address_tag.get_text(strip=True) if address_tag else ''
                        
                        # å„éƒ¨å±‹ã®æƒ…å ±
                        rooms = cassetto.find_all('tbody')
                        
                        for room in rooms:
                            try:
                                # å®¶è³ƒ
                                price_tag = room.find('span', class_='cassetteitem_price--rent')
                                if not price_tag:
                                    continue
                                
                                price_text = price_tag.get_text(strip=True)
                                price = self._extract_number(price_text)
                                
                                if price is None or price < 10000:
                                    continue
                                
                                # ç®¡ç†è²»
                                admin_fee_tag = room.find('span', class_='cassetteitem_price--administration')
                                admin_fee = 0
                                if admin_fee_tag:
                                    admin_text = admin_fee_tag.get_text(strip=True)
                                    if admin_text != '-':
                                        admin_fee = self._extract_number(admin_text) or 0
                                
                                # é–“å–ã‚Š
                                layout_tag = room.find('span', class_='cassetteitem_madori')
                                layout = layout_tag.get_text(strip=True) if layout_tag else ''
                                
                                # é¢ç©
                                area_tag = room.find('span', class_='cassetteitem_menseki')
                                area_size = area_tag.get_text(strip=True) if area_tag else ''
                                
                                properties.append({
                                    'name': name,
                                    'address': address,
                                    'rent': price,
                                    'admin_fee': admin_fee,
                                    'total': price + admin_fee,
                                    'layout': layout,
                                    'area_size': area_size,
                                    'area_name': area_name
                                })
                            
                            except Exception as e:
                                continue
                    
                    except Exception as e:
                        continue
                
                print(f"   âœ… ç´¯è¨ˆ {len(properties)}ä»¶")
            
            except Exception as e:
                print(f"   âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        return properties
    
    def _extract_number(self, text: str) -> float:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º
        ã€Œ8.5ä¸‡å††ã€â†’ 85000
        ã€Œ5000å††ã€â†’ 5000
        """
        # ä¸‡å††è¡¨è¨˜ã®å ´åˆ
        if 'ä¸‡' in text:
            match = re.search(r'([\d.]+)ä¸‡', text)
            if match:
                return float(match.group(1)) * 10000
        
        # é€šå¸¸ã®æ•°å€¤
        text = text.replace(',', '').replace('å††', '')
        numbers = re.findall(r'[\d.]+', text)
        if numbers:
            return float(numbers[0])
        
        return None
    
    def scrape_multiple_areas(self, areas: List[Dict], pages: int = 3) -> List[Dict]:
        """
        è¤‡æ•°ã‚¨ãƒªã‚¢ã‹ã‚‰ä¸€æ‹¬å–å¾—
        
        Args:
            areas: [{'code': '13', 'name': 'æ–°å®¿åŒº'}, ...]
            pages: å„ã‚¨ãƒªã‚¢ã§å–å¾—ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°
            
        Returns:
            å…¨ç‰©ä»¶ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        
        print("="*70)
        print("ğŸ  SUUMO: è³ƒè²¸ç‰©ä»¶ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
        print("="*70)
        print(f"å¯¾è±¡: {len(areas)}ã‚¨ãƒªã‚¢ Ã— {pages}ãƒšãƒ¼ã‚¸")
        print(f"äºˆæƒ³å–å¾—æ™‚é–“: ç´„{len(areas) * pages * 3}ç§’")
        print("="*70)
        
        all_properties = []
        
        for area in areas:
            props = self.scrape_area(area['code'], area['name'], pages)
            
            if props:
                all_properties.extend(props)
                print(f"   âœ… {area['name']}: {len(props)}ä»¶å–å¾—")
            else:
                print(f"   âŒ {area['name']}: å–å¾—å¤±æ•—")
        
        print(f"\n{'='*70}")
        print(f"âœ… åˆè¨ˆ {len(all_properties)}ä»¶å–å¾—å®Œäº†")
        
        # ã‚¨ãƒªã‚¢åˆ¥ä»¶æ•°ç¢ºèª
        from collections import Counter
        area_count = Counter([p['area_name'] for p in all_properties])
        
        print("\nğŸ“Š ã‚¨ãƒªã‚¢åˆ¥å†…è¨³:")
        for area_name, count in area_count.items():
            print(f"   {area_name:10s}: {count}ä»¶")
        
        print(f"{'='*70}")
        
        return all_properties


def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    scraper = SuumoScraper()
    
    # ãƒ†ã‚¹ãƒˆã‚¨ãƒªã‚¢
    test_areas = [
        {'code': '13', 'name': 'æ–°å®¿åŒº'},
        {'code': '13', 'name': 'ä¸–ç”°è°·åŒº'}
    ]
    
    properties = scraper.scrape_multiple_areas(test_areas, pages=1)
    
    if properties:
        print(f"\nã€å–å¾—ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ã€‘")
        for p in properties[:10]:
            print(f"   {p['area_name']:8s} | {p['layout']:6s} | Â¥{p['total']:>8,.0f} | {p['name'][:30]}")
        
        # ã‚¨ãƒªã‚¢åˆ¥å¹³å‡ã‚’ç¢ºèª
        from collections import defaultdict
        area_totals = defaultdict(list)
        
        for p in properties:
            area_totals[p['area_name']].append(p['total'])
        
        print(f"\nã€ã‚¨ãƒªã‚¢åˆ¥å¹³å‡å®¶è³ƒã€‘")
        for area, totals in area_totals.items():
            avg = sum(totals) / len(totals)
            print(f"   {area:8s}: Â¥{avg:>8,.0f} ({len(totals)}ä»¶)")


if __name__ == "__main__":
    main()